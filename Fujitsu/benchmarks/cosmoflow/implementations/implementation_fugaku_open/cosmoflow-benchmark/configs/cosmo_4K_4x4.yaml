output_dir: results/cosmo-000

mlperf:
    org: Fujitsu
    division: open
    status: cloud
    platform: fugaku_{}xA64FX_tensorflow
    poc_name: Koichi Shirahata
    poc_email: k.shirahata@fujitsu.com

data:
    name: cosmo
    data_dir: /data/g9300001/MLPerf/cosmoUniverse_8192_tarfiles_gz
    n_train: 262144
    n_valid: 65536
    sample_shape: [128, 128, 128, 4]
    batch_size: 1
    n_epochs: 200
    shard: True
    apply_log: True
    prefetch: 0
    do_augmentation: True
    use_cache: True
    cache_as_fp32: True

model:
    name: mesh_cosmoflow
    input_shape: [128, 128, 128, 4]
    target_size: 4
    conv_size: 32
    fc1_size: 128
    fc2_size: 64
    hidden_activation: LeakyReLU
    pooling_type: MaxPool3D
    dropout: 0
    mesh_shape: [4,4]

optimizer:
    name: SGDWD
    momentum: 0.9
    #decay: 1.0e-5
    #weight_decay: 1.0e-6
    weight_decay: 0

    #name: LARS
    #momentum: 0.96
    #weight_decay: 1.0e-5
    #epsilon: 1.0e-9

    #name: Adam
    #epsilon: 1.0e-8

lr_schedule:
    # Standard linear LR scaling configuration, tested up to batch size 1024
    base_lr: 0.001
    scaling: linear
    base_batch_size: 64
    #base_batch_size: 16

    # Alternate sqrt LR scaling which has worked well for batch size 512-1024.
    #base_lr: 0.002
    #scaling: sqrt
    #base_batch_size: 32

    n_warmup_epochs: 8
    warmup_factor: 0.1  # initial value is peak_lr * warmup_factor. If negative value is set, initial value is base_lr (traditional behavior)

    # You may want to adjust these decay epochs depending on your batch size.
    # E.g. if training batch size 64 you may want to decay at 16 and 32 epochs.
    decay_schedule:
        #name: step
        #32: 0.25
        #64: 0.125

        name: poly
        n_decay_epochs: 170
        end_factor: 0.01
        power: 1

        #name: cos
        #n_decay_epochs: 140
        #end_factor: 0.009

        #name: htd
        #n_decay_epochs: 140
        #end_factor: 0.1
        #L: -6
        #U: 3

        #name: natan
        #n_decay_epochs: 90
        #end_factor: 0.01
        #turn_epoch: 70

train:
    loss: mse
    metrics: ['mae']
    early_stopping_patience: 8
