= MLPerf HPC Training v0.7 Results Discussion

The following descriptions were provided by the submitting organizations as a supplement to help the public understand the submissions and results. The statements do not reflect the opinions or views of MLCommons. 

== CSCS

The Swiss National Supercomputing Centre (CSCS) participated in the first MLPerf HPC Training round as part of our benchmarking initiative to identify the needs of future systems to support ML workflows in science. We focused on two data-parallel submissions with CosmoFlow on Piz Daint with 128 and 256 GPUs, one GPU per node. By using Sarus, a container engine with near-native performance for Docker-compatible containers, we were able to rapidly test and tune fine-grained communication for distributed training with Horovod and NCCL for near optimal weak scaling in the range of 100-1000 nodes. Curiously, execution time per epoch scaled 12 % faster than ideal from 128 to 256 GPUs. This scaling is a result of being able to cache the data set in RAM with 256 GPUs, whereas with 128 GPUs parallel filesystem I/O becomes an overhead. This overhead could be alleviated using near-compute storage. Algorithmically, our submission demonstrates the limits of CosmoFlow’s data-parallel scalability under closed division rules. Specifically, the number of epochs to converge scales up by about 1.6X as the system scales from 128 to 256 GPUs, while scaling from 32 to 128 GPUs only increases the epoch count by about 1.3X. Additionally, the standard deviation increases by 7X, making the model harder to train. In summary, we have identified fine-grained communication together with the addition of near-compute storage as key optimizations for ML on HPC systems, and CSCS will continue working on alternative parallelization strategies to overcome the data-parallel scalability challenge found in this round.

== Fujitsu

AI Bridging Cloud Infrastructure (ABCI) is the world's first large-scale Open AI Computing Infrastructure, constructed and operated by National Institute of Advanced Industrial Science and Technology (AIST) [1]. The ABCI system is powered by 2,176 Intel Xeon Scalable Processors (Skylake-SP), 4,352 NVIDIA Tesla V100 GPUs, and dual-rail Infiniband EDR interconnects. Fujitsu in collaboration with AIST and Fujitsu Laboratories submitted CosmoFlow and DeepCAM results. For CosmoFlow, 128 nodes (512 GPUs) were used for closed division and 512 nodes (2,048GPUs) were used for open division. The dataset was reformatted to tar.xz files to reduce data staging time, and the following performance optimizations were applied to improve training throughput: (1) improve data loader throughput using NVIDIA Data Loading Library (DALI), (2) apply mixed precision training, (3) increase validation batch size. For open division the following accuracy improvement techniques were also applied: (1) use linear learning rate decay scheduler, (2) apply data augmentation, (3) disable dropout layers. These techniques enabled increasing batch size from 512 to 2,048 and reduced run time by 2.61x. For DeepCAM, 256 nodes (1,024GPUs) were used for closed and open divisions. The dataset was reformatted to tar files to reduce data staging time, and the distributed data shuffles were applied among intra-node multi GPUs, and hyper-parameters were tuned including warmup steps to reduce the number of epochs to convergence. For open division, the Gradient Skipping (GradSkip) technique, one of Content-Aware Computing (CAC) techniques developed by Fujitsu Laboratories, was also applied. GradSkip avoids updating weights in some layers in the training process, by finding layers which have little effect on accuracy, based on automatic analysis of the content of data during the training process. 

[1] https://abci.ai/en/about_abci/

== Fujitsu + RIKEN

RIKEN and Fujitsu are jointly developing the world’s top-level supercomputer—the supercomputer Fugaku—capable of realizing high effective performance for a broad range of application software, with the goal of full operation in 2021 [2]. RIKEN and Fujitsu in collaboration with Fujitsu Laboratories submitted CosmoFlow results for closed division using 512 nodes and 8,192 nodes, and for open division using 16,384 nodes. The dataset was reformatted to tar.xz files for reducing data staging time and LLIO (Lightweight Layered IO Accelerator) was used to make use of temporary local file system in a process. Optimized oneAPI Deep Neural Network Library (oneDNN) was developed to exploit the performance of A64FX. Since the accuracy could not reach the target using the batch size larger than 4,096, the model parallel was introduced to apply hybrid parallelism of both data and model. Model parallel in TensorFlow was extended based on Mesh TensorFlow (MTF) so that multi processes of both data and model parallelisms are enabled. Model parallel was applied in Conv3d layers by spatial partitioning in two dimensions. The hybrid parallelism enabled scaling the number of CPUs up to 8,192 for closed division and 16,384 for open division (about 1/10 of Fugaku).  
[2] https://www.fujitsu.com/global/about/innovation/fugaku/

== LBNL

MLPerf HPC is an important opportunity for the National Energy Research and Scientific Computing (NERSC) center at Lawrence Berkeley National Laboratory as we prepare for a growing scientific AI workload in the coming years. Berkeley Lab co-led the published scientific applications that the current benchmarks are based on, DeepCAM and CosmoFlow. For this first round of results, we have submitted results measured on the Cori supercomputer at NERSC, demonstrating data-parallel training capabilities on both the KNL and GPU partitions up to 1024 nodes and 64 GPUs, respectively. Our participation in MLPerf HPC v0.7 marks an important step for us to standardize our AI benchmarking strategy in preparation for our announced next machine coming online in 2021: Perlmutter.

== NCSA

One of the goals of the Innovative Systems Lab (ISL) at the National Center for Supercomputing Applications (NCSA) is to evaluate emerging hardware and software systems of interest to the AI research community. MLPerf HPC provides a great tool to conduct such evaluations. For this round of benchmarks, we have submitted the results obtained on our Hardware-Accelerated Learning (HAL) cluster based on IBM POWER9 CPUs and NVIDIA V100 GPUs.  The system consists of 16 IBM AC922 nodes backed by an all-flash DDN storage array and EDR InfiniBand interconnect and shows great distributed training capabilities across the entire cluster.  We have developed significant experience while participating in the MLPerf HPC v0.7 project, which will benefit us in our future system designs.

== TACC

The Texas Advanced Computing Center (TACC) designs and operates some of the world's most powerful computing resources. The center's mission is to enable discoveries that advance science and society through the application of advanced computing technologies. MLPerf HPC applications like CosmoFlow provide an invaluable opportunity to understand next-generation ML and DL applications' requirements. TACC participated in MLPerf HPC v0.7 and submitted the performance for the Cosmoflow application at 64 GPUs on the Frontera RTX partition. The lessons learned will be used to derive the architecture of future TACC systems for the benefit of the vast growing AI community.
